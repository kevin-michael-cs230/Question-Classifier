{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import transformers\n",
    "import numpy as np\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from constants import *\n",
    "import dataset_utils as du\n",
    "from new_model import create_model, create_model_no_stackx, create_model_only_stackx\n",
    "from training_metrics import TrainMetrics\n",
    "from evaluation_metrics import EvalMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_name_train(name):\n",
    "    pathstr = name\n",
    "    i = 1\n",
    "    while path.exists(pathstr + '-train.json'):\n",
    "        pathstr = name + str(i)\n",
    "        i += 1\n",
    "    return pathstr\n",
    "\n",
    "def get_next_name_eval(name):\n",
    "    pathstr = name\n",
    "    i = 1\n",
    "    while path.exists(pathstr + '-eval.json'):\n",
    "        pathstr = name + str(i)\n",
    "        i += 1\n",
    "    return pathstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/pandas/core/indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following line to rerun pre-processing and tokenization\n",
    "# prepare_and_save(RAW_DATASET_PATH, path.join(PROCESSED_DIR, PROCESSED_DATASET))\n",
    "\n",
    "data = pd.read_pickle(path.join(PROCESSED_DIR, PROCESSED_DATASET))\n",
    "interest = du.get_interest_dataset(data) \n",
    "interest = du.get_balanced_dataset(interest, 'interest_binary', random_state=1)\n",
    "interest = interest.sample(frac=1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold split\n",
    "features = du.get_features(interest)\n",
    "labels = du.get_labels(interest, 'interest_binary')\n",
    "folds = du.kfold_split(features, labels, num_folds=8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj', 'sequence_summary']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "Epoch 1/6\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification_2/transformer/mask_emb:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_for_sequence_classification_2/transformer/mask_emb:0'] when minimizing the loss.\n",
      "178/348 [==============>...............] - ETA: 1:30 - loss: 0.7049 - accuracy: 0.4547"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "INITIAL_ALPHA = 1e-5\n",
    "LEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(INITIAL_ALPHA, decay_steps=100, decay_rate=0.9, staircase=True)\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 6\n",
    "\n",
    "# train_set, dev_set, test_set = du.split_datasets(interest, TRAIN_RATIO, DEV_RATIO)\n",
    "# train_features, train_labels = du.get_features(train_set), du.get_labels(train_set, 'interest_binary')\n",
    "# dev_features, dev_labels = du.get_features(dev_set), du.get_labels(dev_set, 'interest_binary')\n",
    "# test_features, test_labels = du.get_features(test_set), du.get_labels(test_set, 'interest_binary')\n",
    "\n",
    "train_features = folds[2]['train_features']\n",
    "train_labels = folds[2]['train_labels']\n",
    "\n",
    "\n",
    "name = get_next_name_train('metrics/model_overfit')\n",
    "training_metrics = TrainMetrics(name, epochs=EPOCHS)\n",
    "\n",
    "model = create_model_no_stackx(MAX_LEN, NUM_SX_FEATURES)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(LEARNING_RATE), loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "hist = model.fit(train_features, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True, callbacks=[training_metrics])\n",
    "\n",
    "training_metrics.plot_loss()\n",
    "training_metrics.plot_accuracy()\n",
    "#training_metrics.save_metrics_json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features = folds[2]['test_features']\n",
    "dev_labels = folds[2]['test_labels']\n",
    "\n",
    "dev_preds = model.predict(dev_features, batch_size=BATCH_SIZE)\n",
    "dev_preds = np.rint(dev_preds).astype(int)\n",
    "eval_dataset = {'feats': dev_features, 'labels': dev_labels, 'preds': dev_preds}\n",
    "\n",
    "name = get_next_name_eval('metrics/model_overfit')\n",
    "eval_metrics = EvalMetrics(name, dataset=eval_dataset)\n",
    "eval_metrics.plot_confusion_matrix()\n",
    "print(f'F1 score {eval_metrics.get_f1score()}')\n",
    "print(f'Accuracy {eval_metrics.get_accuracy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_metrics.save_metrics_json()\n",
    "# eval_metrics.save_metrics_json()\n",
    "# import os\n",
    "#os.mkdir('saved_model_weights/FILL_IN')\n",
    "#model.save_weights('saved_model_weights/FILL_IN/v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score0.49638947123223853\n",
      "Accuracy0.559748427672956\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 score{eval_metrics.get_f1score()}')\n",
    "print(f'Accuracy{eval_metrics.get_accuracy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = TrainMetrics('metrics/model_overfit5', load_json=True)\n",
    "training_metrics.plot_accuracy()\n",
    "training_metrics.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.2950187623500824], 'accuracy': [0.8936781883239746]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p37] *",
   "language": "python",
   "name": "conda-env-tensorflow_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

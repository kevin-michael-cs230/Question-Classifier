{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/herrerx/.local/lib/python3.7/site-packages (0.1.95)\n"
     ]
    }
   ],
   "source": [
    "# Install sentencenpiece to help in tokenization\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import transformers\n",
    "import numpy as np\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFXLNetModel, TFAutoModel, TFXLNetForSequenceClassification\n",
    "\n",
    "DATA_DIR = 'datasets'\n",
    "DATASET_PATH = 'labels_with_stackx.csv'\n",
    "TOKENIZED_DATASET_PATH = 'labels_with_stackx_tokenized'\n",
    "TRAIN_SET = 'train_set'\n",
    "DEV_SET = 'dev_set'\n",
    "TEST_SET = 'test_set'\n",
    "\n",
    "TRAIN_RATIO = .6\n",
    "DEV_RATIO = .2\n",
    "TEST_RATIO = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for tokenizing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_dataframe(path: str) -> pd.DataFrame:\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    # Drop all rows with questions that are not understandable\n",
    "    data.drop(data[data.understandable == 0].index, inplace=True)\n",
    "    # All questions are understandable so we can remove the 'understandable' column\n",
    "    data.drop(columns=\"understandable\", inplace=True)\n",
    "    # Remove rows whos passage has more than 'max_len' words\n",
    "    data.drop(data[data.passage.map(lambda x: x.count(\" \") + 1) > MAX_LEN].index, inplace=True)\n",
    "    # Remove rows with a comprehension value of 3, because these values won't work for binary classification\n",
    "    data.drop(data[data.comprehension == 3].index, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create a closure for tokenization\n",
    "\n",
    "def get_tokenize_func():\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased', model_max_length=MAX_LEN)\n",
    "  \n",
    "  def tk(df):\n",
    "    out = tokenizer(df['passage'], df['question'], padding='max_length', truncation=True)\n",
    "    return out['input_ids'], out['token_type_ids'], out['attention_mask']\n",
    "  \n",
    "  return tk\n",
    "\n",
    "def get_tokens(data: pd.DataFrame):\n",
    "    data['output_ids'], data['token_type_ids'], data['attention_mask'] = zip(*data.apply(get_tokenize_func(), axis=1))\n",
    "    for col in ['output_ids', 'token_type_ids', 'attention_mask']:\n",
    "        data[col] = data[col].apply(lambda cell: np.array(cell))\n",
    "    \n",
    "def tokenize_and_save(load_path: str, save_path: str):\n",
    "    data = get_processed_dataframe(load_path)\n",
    "    get_tokens(data)\n",
    "    data.to_pickle(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for splitting a numpy arrays for train, dev and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    # Uncomment the following line of code if you would like to re-run the tokenization process\n",
    "    #tokenize_and_save(path.join(DATA_DIR, DATASET_PATH), path.join(DATA_DIR, TOKENIZED_DATASET_PATH))\n",
    "    \n",
    "    tokenized_data = pd.read_pickle(path.join(DATA_DIR, TOKENIZED_DATASET_PATH))\n",
    "    \n",
    "    # Randomly shuffle the data, but seeded so it's repeatable\n",
    "    data.sample(frac=1, random_state=1)\n",
    "\n",
    "    # Get train, test, dev splits\n",
    "    num_entries = len(tokenized_data)\n",
    "    train_cutoff = int(num_entries * TRAIN_RATIO)\n",
    "    dev_cutoff = train_cutoff + int(num_entries * DEV_RATIO)\n",
    "\n",
    "    train_set = tokenized_data[:train_cutoff]\n",
    "    dev_set = tokenized_data[train_cutoff:dev_cutoff]\n",
    "    test_set = tokenized_data[dev_cutoff:]\n",
    "\n",
    "    train_set.to_pickle(path.join(DATA_DIR, TRAIN_SET))\n",
    "    dev_set.to_pickle(path.join(DATA_DIR, DEV_SET))\n",
    "    test_set.to_pickle(path.join(DATA_DIR,TEST_SET))\n",
    "    \n",
    "def get_features(df: pd.DataFrame) -> list:\n",
    "    output_ids = np.stack(df['output_ids'].values)\n",
    "    token_type_ids = np.stack(df['token_type_ids'].values)\n",
    "    attention_mask = np.stack(df['attention_mask'].values)\n",
    "    return [output_ids, token_type_ids, attention_mask]\n",
    "    \n",
    "def get_labels(df: pd.DataFrame, label_name: str):\n",
    "    # Extract numpy array, and reshape to be rank-2\n",
    "    return np.reshape(df[label_name].values, (-1, 1))\n",
    "    \n",
    "def get_datasets():\n",
    "    train_set = pd.read_pickle(path.join(DATA_DIR, TRAIN_SET))\n",
    "    dev_set = pd.read_pickle(path.join(DATA_DIR, DEV_SET))\n",
    "    test_set = pd.read_pickle(path.join(DATA_DIR,TEST_SET))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_set, dev_set, test_set\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, dev_set, test_set = get_datasets()\n",
    "train_features, train_labels = get_features(train_set), get_labels(train_set, 'comprehension binary')\n",
    "dev_features, dev_labels = get_features(dev_set), get_labels(dev_set, 'comprehension binary')\n",
    "test_features, test_labels = get_features(test_set), get_labels(test_set, 'comprehension binary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p37] *",
   "language": "python",
   "name": "conda-env-tensorflow_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
